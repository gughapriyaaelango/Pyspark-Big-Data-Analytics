{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neAV6tdFTY3U"
      },
      "source": [
        "# Big Data Analytics Homework 02\n",
        "\n",
        "*Complete this assignment in Google Colab. Prior to submitting a copy of this notebook (.ipynb format), run every cell and ensure you have corrected all runtime errors. Be sure to fill in your Name and SUID in the following cell. As always, you must do your own work. This means you may not use answers to the following questions generated by any other person or a generative AI tool such as ChatGPT. You may, however, discuss this assignment with others in a general way and seek help when you need it, but, again, you must do your own work.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2i9feybmTkum"
      },
      "source": [
        "Name: Gughapriyaa Elango\n",
        "\n",
        "SUID: 585992381"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMhlq-j15puN"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uUjg3KNnWWqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6233b0-dd12-442f-c7e6-a2e9f63389e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M4RbOhX6Wbaz"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "spark = SparkSession\\\n",
        "    .builder\\\n",
        "    .appName('Homework 02')\\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifbPI0MyTCnk"
      },
      "source": [
        "## RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5vi8jghS_vJ"
      },
      "source": [
        "### Q1\n",
        "\n",
        "Create a single-dimensional PySpark RDD named `bernoulli_rdd` that contains 1,000 Bernoulli probability distribution data points consisting of integers 0 or 1 with P(0) = P(1) = 0.5.\n",
        "\n",
        "Use only PySpark RDDs and `map` to complete this question.\n",
        "\n",
        "Hint: Use the [`RandomRDDs.uniformRDD`](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.mllib.random.RandomRDDs.html#pyspark.mllib.random.RandomRDDs.uniformRDD) function for your sampling step. But you will still need to map over this RDD with a custom function that creates 1's and 0's from the initial output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Odr2DBddYMI5"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "from pyspark.mllib.random import RandomRDDs\n",
        "\n",
        "#creating bernoulli_rdd containing 1000 bernoulli probability distribn points:\n",
        "bernoulli_rdd = RandomRDDs.uniformRDD(sc, 1000,seed=32)\n",
        "\n",
        "#Function to make 1's and 0's:\n",
        "def bernoulli_int(x):\n",
        "    if x < 0.5:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "#Map over RDD using created function:\n",
        "bernoulli_rdd = bernoulli_rdd.map(bernoulli_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vvFGIsSfbGPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc510b6-3b73-44f8-98d0-b466e9ccef53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# do not modify\n",
        "bernoulli_rdd.take(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ljPLc8MYHFq"
      },
      "source": [
        "### Q2\n",
        "\n",
        "Using only Spark, count and display the number of 1's and the number of 0's in `bernoulli_rdd`.\n",
        "\n",
        "Your output must be of the form \"There are X 1's and Y 0's in bernoulli_rdd\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ey-ybD9_9oxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21270b8a-4bc5-415a-c53e-8b14740b08e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  491 1's and  509 0's in bernoulli_rdd\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "count = bernoulli_rdd.countByValue() #Countbyvalue gives the count of each distinct value\n",
        "print(\"There are \",count[1],\"1's and \",count[0],\"0's in bernoulli_rdd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYmrTabq49RG"
      },
      "source": [
        "### Q3\n",
        "\n",
        "Create a two new 2-dimensional RDD named `bernoulli_sample_rdd_1` and `bernoulli_sample_rdd_2` that each contain sample data from `bernoulli_rdd`. Each element of these RDDs should contain 10 samples (with replacement) from the original 1,000. The length of each RDD should be the number of samples which should be 50. In addition to the samples themselves, each data element in each RDD should contain `sample_number`, which should be calculated from each sample, not \"hard-coded\" as 10.\n",
        "\n",
        "A sample element of the result will be of the form `[(sample_number, [sample])]`.\n",
        "\n",
        "`bernoulli_sample_rdd_1` should be created using the [`sample`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.sample.html) method and `bernoulli_sample_rdd_2` should be created using the[`takeSample`](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.takeSample.html) method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "no = 10 #number of sample\n",
        "frac = no/1000 #fraction for fraction in sample method\n",
        "\n",
        "bernoulli_sample_rdd_1 = [] #initalize empty list\n",
        "for i in range(50):\n",
        "  sample = bernoulli_rdd.sample(withReplacement = True, fraction = frac,seed=i).collect() #get sample\n",
        "  leng = len(bernoulli_rdd.sample(withReplacement = True, fraction = frac,seed=i).collect()) #get length\n",
        "  bernoulli_sample_rdd_1.append([leng,sample]) #create 50 [length,sample]\n",
        "\n",
        "bernoulli_sample_rdd_1 = sc.parallelize(bernoulli_sample_rdd_1) #convert into rdd"
      ],
      "metadata": {
        "id": "-rBdSncCqgc1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bernoulli_sample_rdd_2 = [] #initalize empty list\n",
        "for j in range(50):\n",
        "  sample2 = bernoulli_rdd.takeSample(withReplacement = True, num=no, seed=j) #get sample\n",
        "  leng2 = len(bernoulli_rdd.takeSample(withReplacement = True, num=no, seed=j)) #get length\n",
        "  bernoulli_sample_rdd_2.append([leng2,sample2]) #create 50 [length,sample]\n",
        "\n",
        "bernoulli_sample_rdd_2 = sc.parallelize(bernoulli_sample_rdd_2) #convert into rdd"
      ],
      "metadata": {
        "id": "0Vco2Q2YsDNX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CdYQv3prciZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1907e5d1-b70e-4e1e-cce6-afcfa8f468ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, [0, 0, 1, 0, 0]],\n",
              " [5, [1, 0, 0, 1, 0]],\n",
              " [10, [1, 1, 1, 0, 0, 0, 0, 0, 0, 1]],\n",
              " [10, [1, 0, 1, 1, 0, 0, 0, 0, 1, 0]],\n",
              " [11, [1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0]],\n",
              " [11, [1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1]],\n",
              " [12, [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1]],\n",
              " [12, [1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1]],\n",
              " [10, [1, 1, 0, 1, 0, 0, 0, 0, 0, 1]],\n",
              " [10, [0, 0, 0, 0, 0, 1, 0, 1, 0, 0]]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# do not modify\n",
        "bernoulli_sample_rdd_1.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Ymlvde4wEIss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c887ccd1-9e22-4668-bf74-2d47ecdca97d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[10, [0, 1, 1, 0, 0, 1, 1, 1, 0, 1]],\n",
              " [10, [1, 0, 0, 1, 1, 1, 0, 0, 1, 1]],\n",
              " [10, [1, 1, 1, 1, 1, 0, 1, 0, 0, 0]],\n",
              " [10, [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]],\n",
              " [10, [0, 1, 1, 0, 0, 1, 1, 1, 0, 0]],\n",
              " [10, [1, 0, 0, 0, 0, 0, 0, 1, 1, 0]],\n",
              " [10, [1, 1, 1, 0, 0, 1, 0, 1, 0, 1]],\n",
              " [10, [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]],\n",
              " [10, [0, 1, 1, 0, 1, 1, 1, 0, 1, 0]],\n",
              " [10, [1, 0, 1, 1, 0, 1, 1, 1, 0, 0]]]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# do not modify\n",
        "bernoulli_sample_rdd_2.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62UeDcsJHzRa"
      },
      "source": [
        "### Q4\n",
        "\n",
        "Explain key difference between `bernoulli_sample_rdd_1` and `bernoulli_sample_rdd_2` and the reason for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TSg9k8q6RYe"
      },
      "source": [
        ".sample doesnt returns the exact sample size as intended because internally Spark uses bernounlli sampling for taking the sample. Hence the number of samples doesnt remain fixed, the length varies in each sample. When using .takeSample we can get the exact number of samples as intended which is 10 each time, however this can be take longer time when RDD datasets are large.sample function when set to replacement = True, in the background of Pyspark uses poisson sampling which is a non-deterministic sample size.<br>\n",
        "Reference : https://stackoverflow.com/questions/32837530/how-to-get-a-sample-with-an-exact-sample-size-in-spark-rdd\n",
        "<br>\n",
        "bernoulli_sample_rdd_1 returns an RDD when using .sample and bernoulli_sample_rdd_2 returns a list when using .takesample. Hence bernoulli_sample_rdd_2 needs to be converted into RDD from list in order to use map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxY8j-KsruTt"
      },
      "source": [
        "### Q5\n",
        "\n",
        "Re-using code from above that created `bernoulli_sample_rdd_2`, create `bernoulli_sample_rdd_3` which has 100 observations per sample.\n",
        "\n",
        "Using PySpark `map`, create a new RDD named `bernoulli_sample_mean_rdd` that contains the sampling distribution of the means of the samples contained in `bernoulli_sample_rdd_3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hMV3qgoBIrrU"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "bernoulli_sample_rdd_3 = [] #initialize list\n",
        "for j in range(50):\n",
        "  sample2 = bernoulli_rdd.takeSample(withReplacement = True, num=no, seed=j) #get sample\n",
        "  bernoulli_sample_rdd_3.append([j,sample2]) #element number, sample\n",
        "\n",
        "bernoulli_sample_mean_rdd = sc.parallelize(bernoulli_sample_rdd_3).map(lambda x: (x[0],sum(x[1])/len(x[1])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JPiA2MM7JiMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc7e848-2f70-490a-d65e-3f3f3c12a8a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.6),\n",
              " (1, 0.6),\n",
              " (2, 0.6),\n",
              " (3, 0.6),\n",
              " (4, 0.5),\n",
              " (5, 0.3),\n",
              " (6, 0.6),\n",
              " (7, 0.1),\n",
              " (8, 0.6),\n",
              " (9, 0.6)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# do not modify\n",
        "bernoulli_sample_mean_rdd.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt-MYZymelbp"
      },
      "source": [
        "## DataFrames\n",
        "\n",
        "In this section we will work with data from the U.S. Environmental Protection Agency (EPA). There are two data sets. The first data set consists of daily **temperatures** collected at the U.S. **city** level. The second data set consists of daily **air quality** data collected at the U.S. **county** level. These measurements were taken for the full year 2021."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bOzs4146LDwb"
      },
      "outputs": [],
      "source": [
        "# download the temperature and aqi data sets\n",
        "%%bash\n",
        "\n",
        "if [[ ! -f us-daily-temperatures-2021.csv.csv ]]; then\n",
        " wget https://syr-bda.s3.us-east-2.amazonaws.com/us-daily-temperatures-2021.csv -q\n",
        "fi\n",
        "\n",
        "if [[ ! -f us-daily-aqi-2021.csv.csv ]]; then\n",
        " wget https://syr-bda.s3.us-east-2.amazonaws.com/us-daily-aqi-2021.csv -q\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwX-8dwvCytd"
      },
      "source": [
        "### Q6\n",
        "\n",
        "Load the temperature data (using Spark) into a data frame called `temperature`. Load the air quality data into a data frame called `aqi`. Print the schema and the number of rows for each data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "HgZVlQZQCee-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa6e3cc5-fa15-416d-b6cd-f68eeed2a8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- county: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- sites_reporting: string (nullable = true)\n",
            " |-- mean_temperature_f: string (nullable = true)\n",
            " |-- max_temp_f: string (nullable = true)\n",
            " |-- max_temp_hour: string (nullable = true)\n",
            "\n",
            "Number of rows :  206292\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "temperature = spark.read.format('csv')\\\n",
        "                .option('header', 'true')\\\n",
        "                .option('path', 'us-daily-temperatures-2021.csv')\\\n",
        "                .load()\n",
        "#printSchema\n",
        "temperature.printSchema()\n",
        "print(\"Number of rows : \",temperature.count())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aqi = spark.read.format('csv')\\\n",
        "                .option('header', 'true')\\\n",
        "                .option('path', 'us-daily-aqi-2021.csv')\\\n",
        "                .load()\n",
        "#printSchema\n",
        "aqi.printSchema()\n",
        "print(\"Number of rows : \",aqi.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv-D8RTSvayU",
        "outputId": "5ffc62cd-0b63-4952-f64c-a8becfafb83b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- county: string (nullable = true)\n",
            " |-- aqi: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            "\n",
            "Number of rows :  130922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpdnW2t0JGkW"
      },
      "source": [
        "The `temperature` data is reported at the city level, but the `aqi` data is at the county level. We want the \"grain\" of these data sets to match. This means we need to aggregate the the `temperature` data to the county level. This is tricky because we could possibly have counties in different states with the same name. This means you'll want to aggregate not just at the county level (by date), but at the state *and* county level (by date). We also have 4 different metrics to deal with: the total number of sites, a mean, a max, and one value — `max_temp_hour` — that corresponds to the value of another metric, `max_temp_f`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEYrM7vlG16v"
      },
      "source": [
        "### Q7\n",
        "\n",
        "Create a new data frame called `temperature_county` that contains the **mean** temperature, the **max** temperature, and the **total** sites reporting, for each unique `date`, `state`, and `county` in the `temperature` data frame. The columns `mean_temperature_f`, `max_temp_f`, and `sites_reporting` should retain their names. Additionally, round the **mean** and **max** columns to the nearest whole number and cast them as integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "13UaiI2sJ7M8"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "from pyspark.sql.functions import sum,avg,max\n",
        "from pyspark.sql.types import IntegerType\n",
        "#groupby date, state and county, aggregate using average, sum\n",
        "temperature_county = temperature.groupby('date','state','county').agg(avg('mean_temperature_f').cast(IntegerType()).alias('mean_temperature_f'),max('max_temp_f').cast(IntegerType()).alias('max_temp_f'),sum('sites_reporting').cast(IntegerType()).alias('sites_reporting'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "prkZa3rNJ8cH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d512ea1-6f71-488f-f53f-3c3a75a0f056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in temperature_county: 138555\n",
            "+----------+--------+--------------------+------------------+----------+---------------+\n",
            "|      date|   state|              county|mean_temperature_f|max_temp_f|sites_reporting|\n",
            "+----------+--------+--------------------+------------------+----------+---------------+\n",
            "|2021-01-01| Alabama|            Escambia|                63|        68|              1|\n",
            "|2021-01-01| Alabama|           Jefferson|                65|        73|              2|\n",
            "|2021-01-01|  Alaska|              Denali|                 0|         6|              1|\n",
            "|2021-01-01|  Alaska|Fairbanks North Star|                -9|        -4|              7|\n",
            "|2021-01-01| Arizona|             Cochise|                39|        49|              1|\n",
            "|2021-01-01| Arizona|            Coconino|                29|        35|              1|\n",
            "|2021-01-01| Arizona|            Maricopa|                49|        65|              1|\n",
            "|2021-01-01| Arizona|              Navajo|                29|        42|              1|\n",
            "|2021-01-01| Arizona|                Pima|                46|        63|             10|\n",
            "|2021-01-01|Arkansas|             Pulaski|                41|        45|              1|\n",
            "+----------+--------+--------------------+------------------+----------+---------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not modify\n",
        "print('Rows in temperature_county:', temperature_county.count())\n",
        "temperature_county.orderBy('date', 'state', 'county').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLsQFW_yRh8I"
      },
      "source": [
        "### Q8\n",
        "\n",
        "Create a new data frame called `county_max_temp_hour` that reports the `max_temp_hour` at the same level of aggregation as `temperature_county` in the previous step. This means it should have the **same number of rows** as `temperature_county` and contain the same grouping fields, but only one metric, `max_temp_hour`. Once you have created this data frame, **left join** it to `temperature county` as a new data frame called `temperature_county_final`.\n",
        "\n",
        "I've provided some starter code for you below. Fill in where you see `???` in order to complete the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "FOO26LZBWNnk"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, desc\n",
        "\n",
        "# define a window spec - #create unique combination of state,county,date and order by max temp\n",
        "window_spec = Window.partitionBy('state','county','date')\\\n",
        "  .orderBy(desc('max_temp_f')).\n",
        "\n",
        "# add row number for each group\n",
        "df = temperature\\\n",
        "  .withColumn('row_num', row_number()\\\n",
        "              .over(window_spec))\n",
        "\n",
        "# filter to the first row using row_num\n",
        "df2 = df.filter(fn.col('row_num') == 1)\n",
        "\n",
        "# keep only the grouping columns and max_temp_hour\n",
        "county_max_temp_hour = df2.select('state','county','date','max_temp_hour')\n",
        "\n",
        "# join county_max_temp_hour to temperature_county\n",
        "temperature_county_final = temperature_county.join(county_max_temp_hour,['state','county','date'],'left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9ngrLyD3WRQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2faba3-55ab-4458-a385-b250667598dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in county_max_temp_hour: 138555\n",
            "+--------+--------------------+----------+-------------+\n",
            "|   state|              county|      date|max_temp_hour|\n",
            "+--------+--------------------+----------+-------------+\n",
            "| Alabama|            Escambia|2021-01-01|            0|\n",
            "| Alabama|           Jefferson|2021-01-01|           13|\n",
            "|  Alaska|              Denali|2021-01-01|            4|\n",
            "|  Alaska|Fairbanks North Star|2021-01-01|           14|\n",
            "| Arizona|             Cochise|2021-01-01|           12|\n",
            "| Arizona|            Coconino|2021-01-01|           12|\n",
            "| Arizona|            Maricopa|2021-01-01|           13|\n",
            "| Arizona|              Navajo|2021-01-01|           14|\n",
            "| Arizona|                Pima|2021-01-01|           15|\n",
            "|Arkansas|             Pulaski|2021-01-01|           10|\n",
            "+--------+--------------------+----------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not modify\n",
        "print('Rows in county_max_temp_hour:', county_max_temp_hour.count())\n",
        "county_max_temp_hour.orderBy('date', 'state', 'county').show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "kFdZN_4IeutU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288938fb-f118-4efb-d68a-250012f98aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in temperature_county_final: 138555\n",
            "+--------+--------------------+----------+------------------+----------+---------------+-------------+\n",
            "|   state|              county|      date|mean_temperature_f|max_temp_f|sites_reporting|max_temp_hour|\n",
            "+--------+--------------------+----------+------------------+----------+---------------+-------------+\n",
            "| Alabama|            Escambia|2021-01-01|                63|        68|              1|            0|\n",
            "| Alabama|           Jefferson|2021-01-01|                65|        73|              2|           13|\n",
            "|  Alaska|              Denali|2021-01-01|                 0|         6|              1|            4|\n",
            "|  Alaska|Fairbanks North Star|2021-01-01|                -9|        -4|              7|           14|\n",
            "| Arizona|             Cochise|2021-01-01|                39|        49|              1|           12|\n",
            "| Arizona|            Coconino|2021-01-01|                29|        35|              1|           12|\n",
            "| Arizona|            Maricopa|2021-01-01|                49|        65|              1|           13|\n",
            "| Arizona|              Navajo|2021-01-01|                29|        42|              1|           14|\n",
            "| Arizona|                Pima|2021-01-01|                46|        63|             10|           15|\n",
            "|Arkansas|             Pulaski|2021-01-01|                41|        45|              1|           10|\n",
            "+--------+--------------------+----------+------------------+----------+---------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not modify\n",
        "print('Rows in temperature_county_final:', temperature_county_final.count())\n",
        "temperature_county_final.orderBy('date', 'state', 'county').show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t7rZEs2fDvh"
      },
      "source": [
        "### Q9\n",
        "\n",
        "Join `aqi` to `temperature_county_final` and call the resulting data frame `daily_county_measurements`. Then write code that produces the `date`, name of the `county`, `state`, and `aqi` value where the **highest recorded `aqi`** occurred in 2021. In the event of a tie, take the first instance. Your answer should take the following form:\n",
        "\n",
        "\"The highest recorded AQI value in 2021 occured on [date], in [county] County, [state], and had a value of [aqi].\"\n",
        "\n",
        "I have included some comments and starter code to help you out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "xqw9vnw4mWnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e058259a-ba50-4975-dee3-23495088a2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The highest recorded AQI value in 2021 occured on  2021-09-14 , in  Tulare  County,  California , and had a value of  537 .\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "\n",
        "# join aqi to temperature_county_final\n",
        "# and call the resulting data frame daily_county_measurements\n",
        "daily_county_measurements = temperature_county_final.join(aqi,['state','county','date'],'left')\n",
        "# create a data frame with a single row\n",
        "# that contains the highest recorded AQI value in 2021\n",
        "daily_county_measurements = daily_county_measurements.withColumn(\"aqi\",daily_county_measurements.aqi.cast(IntegerType()))\n",
        "#max_aqi = daily_county_measurements.select(max(fn.col('aqi'))).collect()\n",
        "highest_aqi = daily_county_measurements.agg(fn.max_by('state','aqi').alias('state'),fn.max_by('county','aqi').alias('county'),fn.max('aqi').alias('aqi'),fn.max_by('date','aqi').alias('date')).collect()\n",
        "\n",
        "# extract values from the data frame needed for printing\n",
        "#date = ''.join(row['date'] for row in highest_aqi)\n",
        "\n",
        "# print the output as specified\n",
        "print(\"The highest recorded AQI value in 2021 occured on \",highest_aqi[0].date, \", in \",highest_aqi[0].county,\" County, \",highest_aqi[0].state,\", and had a value of \",highest_aqi[0].aqi,\".\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQjhhS3LpbR"
      },
      "source": [
        "### Q10\n",
        "\n",
        "Using a process similar to Q8, create a new data frame called `highest_temperates_by_state_2021` that contains one row per state, and shows the `date`, `state`, and `max_temp_f` for the **highest recorded temperature** in that state in 2021. In the case of ties, pick the earliest day of the year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "g_i-bD5PzMV8"
      },
      "outputs": [],
      "source": [
        "# your code here - orderby desc max_temp to get maximum temperature\n",
        "# and ascending date to get earliest date in case of ties\n",
        "window_spec = Window.partitionBy('state')\\\n",
        "  .orderBy(desc('max_temp_f'),('date'))\n",
        "#.filter('date like \"2021%\"')\n",
        "df = temperature_county_final.withColumn('row_num', row_number()\\\n",
        "              .over(window_spec))\n",
        "\n",
        "# filter to the first row using row_num\n",
        "highest_temperates_by_state_2021 = df.filter(fn.col('row_num') == 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "EY9gDNeb2LfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add0665d-a827-4337-ac46-3274845b02fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------------+----------+\n",
            "|      date|            state|max_temp_f|\n",
            "+----------+-----------------+----------+\n",
            "|2021-07-10|       California|       124|\n",
            "|2021-06-17|Country Of Mexico|       123|\n",
            "|2021-06-17|          Arizona|       118|\n",
            "|2021-07-10|           Nevada|       118|\n",
            "|2021-06-28|           Oregon|       116|\n",
            "|2021-06-29|       Washington|       116|\n",
            "|2021-06-29|            Idaho|       115|\n",
            "|2021-08-17|           Kansas|       115|\n",
            "|2021-07-10|             Utah|       114|\n",
            "|2021-07-29|      Mississippi|       110|\n",
            "|2021-06-15|          Montana|       110|\n",
            "|2021-06-12|       New Mexico|       110|\n",
            "|2021-06-15|          Wyoming|       109|\n",
            "|2021-06-11|            Texas|       107|\n",
            "|2021-07-09|         Colorado|       105|\n",
            "|2021-06-17|             Iowa|       104|\n",
            "|2021-07-19|     North Dakota|       103|\n",
            "|2021-06-20|         Oklahoma|       103|\n",
            "|2021-06-17|         Nebraska|       102|\n",
            "|2021-07-30|         Arkansas|       101|\n",
            "+----------+-----------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# do not modify\n",
        "highest_temperates_by_state_2021\\\n",
        "  .select('date', 'state', 'max_temp_f')\\\n",
        "  .orderBy(desc('max_temp_f'))\\\n",
        "  .show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}